import torch
import torch.nn as nn
import torch.nn.functional as F
from ultralytics.nn.modules.conv import Conv, RepConv
from ultralytics.nn.modules.block import RepC3

# -----------------------------------------------------------------
# 1. EMA (Efficient Multi-Scale Attention) - 논문 Spec (MatMul 버전)
# -----------------------------------------------------------------
class EMA(nn.Module):
    """
    Paper Eq (3)-(8) & Figure 3: EMA with Cross-Spatial Learning (MatMul)
    """
    def __init__(self, channels, groups=8):
        super(EMA, self).__init__()
        self.groups = groups
        self.c_per_g = channels // groups
        self.channels = channels

        # 1x1 Branch (Encoding H & W)
        self.conv_1x1_h = nn.Conv2d(self.c_per_g, self.c_per_g, (1, 3), padding=(0, 1))
        self.conv_1x1_w = nn.Conv2d(self.c_per_g, self.c_per_g, (3, 1), padding=(1, 0))
        
        # 3x3 Branch (Spatial Structure)
        self.conv_3x3 = nn.Conv2d(self.c_per_g, self.c_per_g, 3, padding=1, groups=self.c_per_g)
        
        # Cross-Spatial Learning Components
        self.gn = nn.GroupNorm(groups, channels) # Group Normalization
        self.sigmoid = nn.Sigmoid()
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x):
        b, c, h, w = x.shape
        # Grouping
        x_group = x.view(b * self.groups, self.c_per_g, h, w)
        
        # --- Parallel Sub-networks Reconstruction ---
        # 1x1 Branch
        x_h = F.adaptive_avg_pool2d(x_group, (h, 1))
        x_w = F.adaptive_avg_pool2d(x_group, (1, w))
        x_h = self.conv_1x1_h(x_h)
        x_w = self.conv_1x1_w(x_w)
        
        # 3x3 Branch
        out_3x3 = self.conv_3x3(x_group) # [B*G, C/G, H, W]

        # --- Cross-Spatial Learning (MatMul Logic) ---
        # Global Info from 1x1 Branch
        # 논문: "global information feature map generated by the 1x1 branch undergoes normalization"
        attn_1x1_combined = x_h + x_w 
        global_info = self.gn(attn_1x1_combined.view(b, c, h, w)).view(b * self.groups, self.c_per_g, h, w)
        
        # Eq (3): 2D Global Avg Pool -> Softmax -> MatMul
        # Vector: [B*G, C/G, 1, 1]
        vec_1x1 = F.adaptive_avg_pool2d(global_info, 1)
        vec_3x3 = F.adaptive_avg_pool2d(out_3x3, 1)
        
        # Softmax for attention weights
        # 논문 Matmul: (1xC/R) * (C/R x HW) 형태의 Attention Map 생성 유도
        # "processed through a softmax operation and undergoes matrix multiplication"
        
        # Reshape for MatMul: [B*G, 1, C/G] x [B*G, C/G, HW] -> [B*G, 1, HW] (Spatial Map)
        vec_1x1_s = self.softmax(vec_1x1.view(b * self.groups, 1, self.c_per_g))
        vec_3x3_s = self.softmax(vec_3x3.view(b * self.groups, 1, self.c_per_g))
        
        flat_3x3 = out_3x3.view(b * self.groups, self.c_per_g, h * w)
        flat_1x1 = global_info.view(b * self.groups, self.c_per_g, h * w)
        
        # MatMul 1: Global(1x1) Vector * Spatial(3x3) Map
        spatial_att_1 = torch.matmul(vec_1x1_s, flat_3x3).view(b * self.groups, 1, h, w)
        
        # MatMul 2: Global(3x3) Vector * Spatial(1x1) Map
        spatial_att_2 = torch.matmul(vec_3x3_s, flat_1x1).view(b * self.groups, 1, h, w)
        
        # Final Attention Map
        final_att = self.sigmoid(spatial_att_1 + spatial_att_2)
        
        # Apply Attention to each group
        out = x_group * final_att
        return out.view(b, c, h, w)

# -----------------------------------------------------------------
# 2. ECPConv & ECPConvBlock (Backbone)
# -----------------------------------------------------------------
class ECPConv(nn.Module):
    """
    Paper Figure 2 & Algorithm A1: Enhanced Channel-wise Partial Convolution with Dynamic Selection.
    Core Logic (Following Paper Exactly):
    1. Calculate channel weights via GAP + FC layers.
    2. Sort and select top-k channels dynamically.
    3. Process top-k with RepConv (standard conv, g=1).
    4. Restore processed channels to original index positions.
    """
    def __init__(self, c1, c2, k=3, s=1, ratio=0.25):
        super().__init__()
        self.c1 = c1
        self.c2 = c2
        self.s = s
        # ratio에 따라 RepConv로 보낼 채널 수(k_channels) 결정 (논문 권장: C/4)
        self.k_channels = int(c1 * ratio) 
        self.rest_channels = c1 - self.k_channels
        
        # 1. Channel Selection Network (Weight Generator)
        # 논문: GAP -> FC1 -> ReLU -> FC2 -> Sigmoid
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.selection_net = nn.Sequential(
            nn.Conv2d(c1, c1 // 16, 1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(c1 // 16, c1, 1, bias=False),
            nn.Sigmoid()
        )
        
        # 2. Processing Modules
        # 논문 Algorithm A1: RepConv on selected channels (일반 Conv, g=1)
        # 수정: g 파라미터 제거하여 채널 간 상호작용 허용
        self.rep_conv = RepConv(self.k_channels, self.k_channels, k, s, p=k//2)
        
        # 논문 3.4절: "unselected channels... utilized by two 1×1 convolutions
        # of the intermediate entrainment BN and ReLU functions"
        self.rest_conv = nn.Sequential(
            Conv(c1, c1, 1, s, act=True),   # 1x1 Conv + BN + ReLU (stride 적용)
            Conv(c1, c1, 1, 1, act=False)   # 1x1 Conv (no activation)
        )
        
        # 3. Channel Projection (c1 -> c2)
        self.proj = Conv(c1, c2, 1, 1) if c1 != c2 else nn.Identity()

    def forward(self, x):
        B, C, H, W = x.shape
        
        # -------------------------------------------------------
        # Step 1: Generate Channel Weights (논문 Algorithm A1, Line 3-8)
        # -------------------------------------------------------
        w = self.selection_net(self.avg_pool(x)).view(B, C)
        
        # -------------------------------------------------------
        # Step 2: Sort & Select Top-K (논문 Algorithm A1, Line 9-12)
        # -------------------------------------------------------
        _, topk_idx = torch.topk(w, self.k_channels, dim=1)
        
        # 배치 인덱싱을 위한 준비
        batch_idx = torch.arange(B, device=x.device).unsqueeze(1).expand(-1, self.k_channels)
        
        # 선택된 채널 추출
        x_selected = x[batch_idx, topk_idx]  # [B, k_channels, H, W]
        
        # -------------------------------------------------------
        # Step 3: RepConv on Selected Channels (논문 Algorithm A1, Line 13)
        # -------------------------------------------------------
        y_selected = self.rep_conv(x_selected)  # [B, k_channels, H_out, W_out]
        
        # stride > 1인 경우 출력 크기 계산
        H_out, W_out = y_selected.shape[2], y_selected.shape[3]
        
        # -------------------------------------------------------
        # Step 4: Restore to Original Positions (논문 Algorithm A1, Line 14)
        # X[Y[k]] = Z[k] - 원래 인덱스 위치에 삽입
        # -------------------------------------------------------
        # 나머지 채널에 1x1 Conv 2개 적용 (논문 3.4절)
        out = self.rest_conv(x)
        
        # 선택된 채널 위치에 처리된 결과 삽입 (scatter 연산)
        # topk_idx를 H, W 차원에 맞게 확장
        topk_idx_expanded = topk_idx.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, H_out, W_out)
        
        # scatter를 사용하여 원래 위치에 삽입
        out.scatter_(1, topk_idx_expanded, y_selected)
        
        # Channel projection to c2
        return self.proj(out)

class ECPConvBlock(nn.Module):
    """
    Paper Figure 4: ECPConvBlock (Replaces ResNetBlock)
    Structure: Input -> ECPConv -> (Residual) -> EMA -> Output
    """
    def __init__(self, c1, c2, s=1, e=4): # e is expansion, not used much in R18 but kept for compat
        super().__init__()
        # ResNet-18 style BasicBlock with ECPConv
        # Input c1 -> ECPConv(3x3) -> Conv(1x1) -> EMA
        # 논문의 구조에 맞게 조정: ECPConv가 3x3 역할 수행
        self.ecp_conv = ECPConv(c1, c2, k=3, s=s) 
        self.bn_relu = nn.Sequential(nn.BatchNorm2d(c2), nn.ReLU(inplace=True))
        
        # EMA Module
        self.ema = EMA(c2)
        
        # Shortcut (Downsample if needed)
        self.shortcut = nn.Identity()
        if s != 1 or c1 != c2:
            self.shortcut = Conv(c1, c2, 1, s, act=False)

    def forward(self, x):
        identity = self.shortcut(x)
        
        out = self.ecp_conv(x)
        out = self.bn_relu(out)
        
        out = out + identity
        out = self.ema(out) # EMA enhances the result
        return out

# -----------------------------------------------------------------
# 3. RCM (Rectangular Self-Calibration Module) - Neck Component
# -----------------------------------------------------------------
class RCM(nn.Module):
    """
    Paper Figure 6 & Eq(5): Rectangular Self-Calibration Module
    논문 Eq(5): Qc = sigmoid(SConv_{k×1}(relu(BN(SConv_{1×k}(y)))))
    순차 처리: 1×k (가로) → BN → ReLU → k×1 (세로) → Sigmoid
    """
    def __init__(self, c1, c2, k=11): # k is strip conv kernel size
        super().__init__()
        self.dw_conv = Conv(c1, c1, 3, 1, g=c1, act=False) # DWConv 3x3
        
        # Horizontal & Vertical Pooling (Eq 4)
        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))
        self.pool_w = nn.AdaptiveAvgPool2d((1, None))
        
        # Strip Convolutions - 논문 Eq(5) 순차 처리
        # SConv_{1×k} → BN → ReLU → SConv_{k×1} → Sigmoid
        self.sconv_1xk = Conv(c1, c1, (1, k), 1, act=True)   # 1×k + BN + ReLU
        self.sconv_kx1 = Conv(c1, c1, (k, 1), 1, act=False)  # k×1 (no activation, sigmoid later)
        
        self.sigmoid = nn.Sigmoid()
        self.mlp = nn.Sequential(
            Conv(c1, c2, 1, 1, act=True),
            Conv(c2, c2, 1, 1, act=False)
        )

    def forward(self, x):
        # RCA Process
        identity = x
        
        # Eq(4): Context info via pooling
        x_h = self.pool_h(x)  # [B, C, H, 1]
        x_w = self.pool_w(x)  # [B, C, 1, W]
        y = x_h + x_w  # Broadcast addition [B, C, H, W]
        
        # Eq(5): Self-Calibration Function - 순차 처리 (논문 수식 그대로)
        # Qc = sigmoid(SConv_{k×1}(relu(BN(SConv_{1×k}(y)))))
        qc = self.sigmoid(self.sconv_kx1(self.sconv_1xk(y)))
        
        # Eq(6): Fusion - DWConv output과 Qc의 Hadamard product
        z = self.dw_conv(x) * qc
        
        # Eq(7): Final Output with residual
        return self.mlp(z) + identity

# -----------------------------------------------------------------
# 4. MFFPN (Neck) - Replaces HybridEncoder
# -----------------------------------------------------------------
class SmokeMFFPN(nn.Module):
    """
    Paper Figure 5: Multi-Scale Foreground-Focus Fusion Pyramid Network
    Replaces RT-DETR's HybridEncoder.
    Input: [S3, S4, F5] (Note: F5 is from AIFI, handled by RT-DETR controller or included here)
    """
    def __init__(self, in_channels=[256, 256, 256], hidden_dim=256):
        super().__init__()
        # Input channels are expected to be projected to hidden_dim before this or inside
        self.in_channels = in_channels
        c3, c4, c5 = in_channels
        
        # RCM Modules for Multi-scale Enhancement (Fig 7)
        self.rcm_fused = nn.Sequential(RCM(hidden_dim*3, hidden_dim*3), RCM(hidden_dim*3, hidden_dim*3), RCM(hidden_dim*3, hidden_dim*3))
        
        # RCM Modules for Independent Feature Fusion (Fig 5 Left)
        self.rcm_s3 = RCM(hidden_dim, hidden_dim)
        self.rcm_s4 = RCM(hidden_dim, hidden_dim)
        self.rcm_f5 = RCM(hidden_dim, hidden_dim)
        
        # RepC3 Blocks for Final Output Generation (Fig 5 Right)
        self.rep_c3_p4 = RepC3(hidden_dim*2, hidden_dim, n=3)
        self.rep_c3_p5 = RepC3(hidden_dim*2, hidden_dim, n=3)
        
        # Convolutions for transitions
        self.conv_p3 = Conv(hidden_dim, hidden_dim, 1, 1)
        self.conv_p4 = Conv(hidden_dim, hidden_dim, 1, 1)
        self.conv_p5 = Conv(hidden_dim, hidden_dim, 1, 1)
        
        self.downsample_c1 = Conv(hidden_dim, hidden_dim, 3, 2)
        self.downsample_c2 = Conv(hidden_dim, hidden_dim, 3, 2)

    def forward(self, feats):
        # feats: [S3, S4, F5] - Assuming channel proj is done by AIFI/P5 wrapper in YAML
        s3, s4, f5 = feats 
        
        # 1. Multi-Scale Feature Foreground Enhancement (Fig 7)
        # Downsample to common size (smallest, F5 size)
        s3_d = F.adaptive_avg_pool2d(s3, f5.shape[2:])
        s4_d = F.adaptive_avg_pool2d(s4, f5.shape[2:])
        f5_d = f5
        
        concat_feats = torch.cat([s3_d, s4_d, f5_d], dim=1) # [B, 3C, H, W]
        enhanced_feats = self.rcm_fused(concat_feats)
        s3_prime, s4_prime, f5_prime = torch.chunk(enhanced_feats, 3, dim=1)
        
        # 2. Foreground Feature Fusion (Eq 9, 10, 11)
        # P5 calculation
        p5_base = self.rcm_f5(f5)
        p5 = p5_base * f5_prime # Interpolated multiplication (sizes match here)
        
        # P4 calculation
        p5_up = F.interpolate(p5, size=s4.shape[2:], mode='bilinear')
        s4_base = self.rcm_s4(s4)
        s4_prime_up = F.interpolate(s4_prime, size=s4.shape[2:], mode='bilinear')
        p4 = (s4_base + p5_up) * s4_prime_up
        
        # P3 calculation
        p4_up = F.interpolate(p4, size=s3.shape[2:], mode='bilinear')
        s3_base = self.rcm_s3(s3)
        s3_prime_up = F.interpolate(s3_prime, size=s3.shape[2:], mode='bilinear')
        p3 = (s3_base + p4_up) * s3_prime_up
        
        # 3. Output Generation (Eq 12-15)
        # C1 (from P3)
        c1 = self.conv_p3(p3)
        
        # C2 (Merge C1_down and P4)
        c1_down = self.downsample_c1(c1)
        c2_in = torch.cat([c1_down, p4], dim=1)
        c2 = self.conv_p4(self.rep_c3_p4(c2_in))
        
        # C3 (Merge C2_down and P5)
        c2_down = self.downsample_c2(c2)
        c3_in = torch.cat([c2_down, p5], dim=1)
        c3 = self.conv_p5(self.rep_c3_p5(c3_in))
        
        return [c1, c2, c3] # Returns multi-scale features for Head