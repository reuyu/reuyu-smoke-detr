import torch
import torch.nn as nn
import torch.nn.functional as F
from ultralytics.nn.modules.conv import Conv, RepConv
from ultralytics.nn.modules.block import RepC3

# -----------------------------------------------------------------
# 1. EMA (Efficient Multi-Scale Attention) - 논문 Spec (MatMul 버전)
# -----------------------------------------------------------------
class EMA(nn.Module):
    """
    Paper Eq (3)-(8) & Figure 3: EMA with Cross-Spatial Learning (MatMul)
    """
    def __init__(self, channels, groups=8):
        super(EMA, self).__init__()
        self.groups = groups
        self.c_per_g = channels // groups
        self.channels = channels

        # 1x1 Branch (Encoding H & W)
        self.conv_1x1_h = nn.Conv2d(self.c_per_g, self.c_per_g, (1, 3), padding=(0, 1))
        self.conv_1x1_w = nn.Conv2d(self.c_per_g, self.c_per_g, (3, 1), padding=(1, 0))
        
        # 3x3 Branch (Spatial Structure)
        self.conv_3x3 = nn.Conv2d(self.c_per_g, self.c_per_g, 3, padding=1, groups=self.c_per_g)
        
        # Cross-Spatial Learning Components
        self.gn = nn.GroupNorm(groups, channels) # Group Normalization
        self.sigmoid = nn.Sigmoid()
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x):
        b, c, h, w = x.shape
        # Grouping
        x_group = x.view(b * self.groups, self.c_per_g, h, w)
        
        # --- Parallel Sub-networks Reconstruction ---
        # 1x1 Branch
        x_h = F.adaptive_avg_pool2d(x_group, (h, 1))
        x_w = F.adaptive_avg_pool2d(x_group, (1, w))
        x_h = self.conv_1x1_h(x_h)
        x_w = self.conv_1x1_w(x_w)
        
        # 3x3 Branch
        out_3x3 = self.conv_3x3(x_group) # [B*G, C/G, H, W]

        # --- Cross-Spatial Learning (MatMul Logic) ---
        # Global Info from 1x1 Branch
        # 논문: "global information feature map generated by the 1x1 branch undergoes normalization"
        attn_1x1_combined = x_h + x_w 
        global_info = self.gn(attn_1x1_combined.view(b, c, h, w)).view(b * self.groups, self.c_per_g, h, w)
        
        # Eq (3): 2D Global Avg Pool -> Softmax -> MatMul
        # Vector: [B*G, C/G, 1, 1]
        vec_1x1 = F.adaptive_avg_pool2d(global_info, 1)
        vec_3x3 = F.adaptive_avg_pool2d(out_3x3, 1)
        
        # Softmax for attention weights
        # 논문 Matmul: (1xC/R) * (C/R x HW) 형태의 Attention Map 생성 유도
        # "processed through a softmax operation and undergoes matrix multiplication"
        
        # Reshape for MatMul: [B*G, 1, C/G] x [B*G, C/G, HW] -> [B*G, 1, HW] (Spatial Map)
        vec_1x1_s = self.softmax(vec_1x1.view(b * self.groups, 1, self.c_per_g))
        vec_3x3_s = self.softmax(vec_3x3.view(b * self.groups, 1, self.c_per_g))
        
        flat_3x3 = out_3x3.view(b * self.groups, self.c_per_g, h * w)
        flat_1x1 = global_info.view(b * self.groups, self.c_per_g, h * w)
        
        # MatMul 1: Global(1x1) Vector * Spatial(3x3) Map
        spatial_att_1 = torch.matmul(vec_1x1_s, flat_3x3).view(b * self.groups, 1, h, w)
        
        # MatMul 2: Global(3x3) Vector * Spatial(1x1) Map
        spatial_att_2 = torch.matmul(vec_3x3_s, flat_1x1).view(b * self.groups, 1, h, w)
        
        # Final Attention Map
        final_att = self.sigmoid(spatial_att_1 + spatial_att_2)
        
        # Apply Attention to each group
        out = x_group * final_att
        return out.view(b, c, h, w)

# -----------------------------------------------------------------
# 2. ECPConv & ECPConvBlock (Backbone)
# -----------------------------------------------------------------
class ECPConv(nn.Module):
    """
    Paper Figure 2: Enhanced Channel-wise Partial Convolution with Dynamic Selection.
    Core Logic:
    1. Calculate channel weights.
    2. Sort and select top-k channels dynamically.
    3. Process top-k with RepConv, rest with 1x1 Conv.
    """
    def __init__(self, c1, c2, k=3, s=1, ratio=0.25):
        super().__init__()
        self.c1 = c1
        self.c2 = c2
        self.s = s  # stride 저장
        # ratio에 따라 RepConv로 보낼 채널 수(k_channels) 결정
        self.k_channels = int(c1 * ratio) 
        self.rest_channels = c1 - self.k_channels
        
        # 1. Channel Selection Network (Weight Generator)
        # GAP -> FC -> ReLU -> FC -> Sigmoid
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.selection_net = nn.Sequential(
            nn.Conv2d(c1, c1 // 16, 1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(c1 // 16, c1, 1, bias=False),
            nn.Sigmoid()
        )
        
        # 2. Processing Modules
        # Selected channels -> RepConv (Complex spatial features)
        self.rep_conv = RepConv(self.k_channels, self.k_channels, k, s, p=k//2, g=self.k_channels)
        
        # Remaining channels -> 1x1 Conv (Simple feature preservation)
        # stride를 동일하게 적용하여 출력 크기 일치
        if s == 1:
            self.rest_conv = Conv(self.rest_channels, self.rest_channels, 1, 1)
        else:
            # stride > 1일 때는 3x3 conv로 다운샘플링
            self.rest_conv = Conv(self.rest_channels, self.rest_channels, 3, s)
        
        # 3. Channel Projection (c1 -> c2)
        # ECPConv 후 출력 채널을 c2로 변환
        self.proj = Conv(c1, c2, 1, 1) if c1 != c2 else nn.Identity()

    def forward(self, x):
        B, C, H, W = x.shape
        
        # -------------------------------------------------------
        # Step 1: Generate Channel Weights
        # -------------------------------------------------------
        # w: [B, C, 1, 1] -> [B, C]
        w = self.selection_net(self.avg_pool(x)).view(B, C)
        
        # -------------------------------------------------------
        # Step 2: Dynamic Selection (Sort & Split)
        # -------------------------------------------------------
        # 상위 k개의 인덱스를 추출 (Indices of top-k importance)
        # topk_idx: [B, k_channels]
        _, topk_idx = torch.topk(w, self.k_channels, dim=1)
        
        # Gather를 위해 배치 차원 인덱스 생성
        # batch_idx: [B, 1] -> [B, k_channels]로 확장
        batch_idx = torch.arange(B, device=x.device).unsqueeze(1).expand(-1, self.k_channels)
        
        # A. 선택된 채널 (Selected Channels) 추출
        # x: [B, C, H, W] -> x_selected: [B, k_channels, H, W]
        x_selected = x[batch_idx, topk_idx]
        
        # B. 나머지 채널 (Rest Channels) 추출
        # 마스킹 기법을 사용하여 선택되지 않은 인덱스만 필터링
        mask = torch.zeros(B, C, dtype=torch.bool, device=x.device)
        # scatter_로 topk 위치를 True로 마킹
        mask.scatter_(1, topk_idx, True) 
        # ~mask(False인 곳)를 선택하여 가져옴. 
        # view를 통해 [B, rest_channels, H, W] 형태로 복원
        x_rest = x[~mask].view(B, self.rest_channels, H, W)
        
        # -------------------------------------------------------
        # Step 3: Independent Processing
        # -------------------------------------------------------
        # 중요한 채널은 구조적 특징 학습 (RepConv)
        y_selected = self.rep_conv(x_selected)
        
        # 덜 중요한 채널은 정보 유지 (1x1 Conv)
        y_rest = self.rest_conv(x_rest)
        
        # -------------------------------------------------------
        # Step 4: Fusion
        # -------------------------------------------------------
        # 원래 채널 순서로 복구할 필요 없이 피처를 합쳐서 다음 레이어로 전달
        # (CNN은 채널 순서보다 채널 내의 정보가 중요하므로 Concat만으로 충분)
        out = torch.cat([y_selected, y_rest], dim=1)
        
        # Channel projection to c2
        return self.proj(out)

class ECPConvBlock(nn.Module):
    """
    Paper Figure 4: ECPConvBlock (Replaces ResNetBlock)
    Structure: Input -> ECPConv -> (Residual) -> EMA -> Output
    """
    def __init__(self, c1, c2, s=1, e=4): # e is expansion, not used much in R18 but kept for compat
        super().__init__()
        # ResNet-18 style BasicBlock with ECPConv
        # Input c1 -> ECPConv(3x3) -> Conv(1x1) -> EMA
        # 논문의 구조에 맞게 조정: ECPConv가 3x3 역할 수행
        self.ecp_conv = ECPConv(c1, c2, k=3, s=s) 
        self.bn_relu = nn.Sequential(nn.BatchNorm2d(c2), nn.ReLU(inplace=True))
        
        # EMA Module
        self.ema = EMA(c2)
        
        # Shortcut (Downsample if needed)
        self.shortcut = nn.Identity()
        if s != 1 or c1 != c2:
            self.shortcut = Conv(c1, c2, 1, s, act=False)

    def forward(self, x):
        identity = self.shortcut(x)
        
        out = self.ecp_conv(x)
        out = self.bn_relu(out)
        
        out = out + identity
        out = self.ema(out) # EMA enhances the result
        return out

# -----------------------------------------------------------------
# 3. RCM (Rectangular Self-Calibration Module) - Neck Component
# -----------------------------------------------------------------
class RCM(nn.Module):
    """ Paper Figure 6: Rectangular Self-Calibration Module """
    def __init__(self, c1, c2, k=11): # k is strip conv kernel size
        super().__init__()
        self.dw_conv = Conv(c1, c1, 3, 1, g=c1, act=False) # DWConv 3x3
        
        # Horizontal & Vertical Pooling (Eq 4)
        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))
        self.pool_w = nn.AdaptiveAvgPool2d((1, None))
        
        # Strip Convolutions (Eq 5)
        # H-direction: 1xK
        self.sconv_h_1 = Conv(c1, c1, (1, k), 1, act=True) # BN+ReLU included in Conv
        self.sconv_h_2 = Conv(c1, c1, (k, 1), 1, act=False)
        
        # V-direction
        self.sconv_v_1 = Conv(c1, c1, (k, 1), 1, act=True)
        self.sconv_v_2 = Conv(c1, c1, (1, k), 1, act=False)
        
        self.sigmoid = nn.Sigmoid()
        self.mlp = nn.Sequential(
            Conv(c1, c2, 1, 1, act=True),
            Conv(c2, c2, 1, 1, act=False)
        )

    def forward(self, x):
        # RCA Process
        identity = x
        
        # Context info
        x_h = self.pool_h(x)
        x_w = self.pool_w(x)
        y = x_h + x_w # Broadcast addition
        
        # Self-Calibration Function (Eq 5)
        # Horizontal Branch
        q_h = self.sconv_h_2(self.sconv_h_1(y))
        # Vertical Branch
        q_v = self.sconv_v_2(self.sconv_v_1(y))
        
        qc = self.sigmoid(q_h + q_v)
        
        # Fusion (Eq 6)
        z = self.dw_conv(x) * qc
        
        # Final Output (Eq 7)
        return self.mlp(z) + identity

# -----------------------------------------------------------------
# 4. MFFPN (Neck) - Replaces HybridEncoder
# -----------------------------------------------------------------
class SmokeMFFPN(nn.Module):
    """
    Paper Figure 5: Multi-Scale Foreground-Focus Fusion Pyramid Network
    Replaces RT-DETR's HybridEncoder.
    Input: [S3, S4, F5] (Note: F5 is from AIFI, handled by RT-DETR controller or included here)
    """
    def __init__(self, in_channels=[256, 256, 256], hidden_dim=256):
        super().__init__()
        # Input channels are expected to be projected to hidden_dim before this or inside
        self.in_channels = in_channels
        c3, c4, c5 = in_channels
        
        # RCM Modules for Multi-scale Enhancement (Fig 7)
        self.rcm_fused = nn.Sequential(RCM(hidden_dim*3, hidden_dim*3), RCM(hidden_dim*3, hidden_dim*3), RCM(hidden_dim*3, hidden_dim*3))
        
        # RCM Modules for Independent Feature Fusion (Fig 5 Left)
        self.rcm_s3 = RCM(hidden_dim, hidden_dim)
        self.rcm_s4 = RCM(hidden_dim, hidden_dim)
        self.rcm_f5 = RCM(hidden_dim, hidden_dim)
        
        # RepC3 Blocks for Final Output Generation (Fig 5 Right)
        self.rep_c3_p4 = RepC3(hidden_dim*2, hidden_dim, n=3)
        self.rep_c3_p5 = RepC3(hidden_dim*2, hidden_dim, n=3)
        
        # Convolutions for transitions
        self.conv_p3 = Conv(hidden_dim, hidden_dim, 1, 1)
        self.conv_p4 = Conv(hidden_dim, hidden_dim, 1, 1)
        self.conv_p5 = Conv(hidden_dim, hidden_dim, 1, 1)
        
        self.downsample_c1 = Conv(hidden_dim, hidden_dim, 3, 2)
        self.downsample_c2 = Conv(hidden_dim, hidden_dim, 3, 2)

    def forward(self, feats):
        # feats: [S3, S4, F5] - Assuming channel proj is done by AIFI/P5 wrapper in YAML
        s3, s4, f5 = feats 
        
        # 1. Multi-Scale Feature Foreground Enhancement (Fig 7)
        # Downsample to common size (smallest, F5 size)
        s3_d = F.adaptive_avg_pool2d(s3, f5.shape[2:])
        s4_d = F.adaptive_avg_pool2d(s4, f5.shape[2:])
        f5_d = f5
        
        concat_feats = torch.cat([s3_d, s4_d, f5_d], dim=1) # [B, 3C, H, W]
        enhanced_feats = self.rcm_fused(concat_feats)
        s3_prime, s4_prime, f5_prime = torch.chunk(enhanced_feats, 3, dim=1)
        
        # 2. Foreground Feature Fusion (Eq 9, 10, 11)
        # P5 calculation
        p5_base = self.rcm_f5(f5)
        p5 = p5_base * f5_prime # Interpolated multiplication (sizes match here)
        
        # P4 calculation
        p5_up = F.interpolate(p5, size=s4.shape[2:], mode='bilinear')
        s4_base = self.rcm_s4(s4)
        s4_prime_up = F.interpolate(s4_prime, size=s4.shape[2:], mode='bilinear')
        p4 = (s4_base + p5_up) * s4_prime_up
        
        # P3 calculation
        p4_up = F.interpolate(p4, size=s3.shape[2:], mode='bilinear')
        s3_base = self.rcm_s3(s3)
        s3_prime_up = F.interpolate(s3_prime, size=s3.shape[2:], mode='bilinear')
        p3 = (s3_base + p4_up) * s3_prime_up
        
        # 3. Output Generation (Eq 12-15)
        # C1 (from P3)
        c1 = self.conv_p3(p3)
        
        # C2 (Merge C1_down and P4)
        c1_down = self.downsample_c1(c1)
        c2_in = torch.cat([c1_down, p4], dim=1)
        c2 = self.conv_p4(self.rep_c3_p4(c2_in))
        
        # C3 (Merge C2_down and P5)
        c2_down = self.downsample_c2(c2)
        c3_in = torch.cat([c2_down, p5], dim=1)
        c3 = self.conv_p5(self.rep_c3_p5(c3_in))
        
        return [c1, c2, c3] # Returns multi-scale features for Head